{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b360890a",
   "metadata": {},
   "source": [
    "# Prepara√ß√£o de Dados (N1)\n",
    "\n",
    "## üóÇÔ∏è [ Projeto ]\n",
    "\n",
    "**Nome do Projeto:** [nome do projeto]  \n",
    "**Dataset:** [Nome do dataset ou fonte de dados]  \n",
    "**Objetivo Geral:** [Breve Explica√ß√£o do prop√≥sito ‚Äî ex.: ‚ÄúAnalisar o comportamento de churn e identificar fatores de reten√ß√£o de clientes.‚Äù]  \n",
    "\n",
    "---\n",
    "\n",
    "## üìã Descri√ß√£o Geral do Tratamento\n",
    "\n",
    "Nesta etapa ser√£o realizados os principais **processos de limpeza, transforma√ß√£o e padroniza√ß√£o dos dados**.  \n",
    "O foco √© garantir que o dataset esteja **consistente, estruturado e pronto para an√°lises posteriores**.\n",
    "\n",
    "**Tarefas previstas:**  \n",
    "- Leitura e diagn√≥stico inicial do dataset.  \n",
    "- Tratamento de valores nulos, duplicados e outliers.  \n",
    "- Convers√£o de tipos e padroniza√ß√£o de nomes de colunas.  \n",
    "- Codifica√ß√£o de vari√°veis categ√≥ricas e normaliza√ß√£o de num√©ricas (quando necess√°rio).  \n",
    "- Gera√ß√£o de artefatos intermedi√°rios e relat√≥rio de qualidade dos dados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c91f747",
   "metadata": {},
   "source": [
    "## üîß Configura√ß√£o do Projeto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1233a7d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Config carregada de: C:\\Users\\fabio\\Projetos DEV\\data projects\\data-project-template-base\\config\\defaults.json\n",
      "[INFO] Overrides locais: C:\\Users\\fabio\\Projetos DEV\\data projects\\data-project-template-base\\config\\local.json\n",
      "2025-10-27 05:39:45,652 | INFO | Project configuration loaded.\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# Configura√ß√µes do projeto\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "import sys\n",
    "import json\n",
    "from typing import Dict, Any, Optional\n",
    "\n",
    "# ---- Util: busca \"para cima\" at√© achar o caminho relativo solicitado ----\n",
    "def find_upwards(relative_path: str, start: Optional[Path] = None) -> Optional[Path]:\n",
    "    \"\"\"\n",
    "    Procura por 'relative_path' subindo diret√≥rios a partir de 'start' (ou CWD).\n",
    "    Retorna o Path encontrado ou None se n√£o existir.\n",
    "    \"\"\"\n",
    "    start = start or Path.cwd()\n",
    "    rel_parts = Path(relative_path).parts\n",
    "    for base in (start, *start.parents):\n",
    "        candidate = base.joinpath(*rel_parts)\n",
    "        if candidate.exists():\n",
    "            return candidate\n",
    "    return None\n",
    "\n",
    "# ---- Carregamento de configura√ß√£o (defaults + overrides locais) ----\n",
    "def load_config(base_rel=\"config/defaults.json\", local_rel=\"config/local.json\") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Carrega as configura√ß√µes do projeto:\n",
    "    - defaults.json (obrigat√≥rio na raiz do projeto)\n",
    "    - local.json (opcional; sobrescreve chaves do defaults)\n",
    "    A busca √© feita subindo diret√≥rios a partir do CWD.\n",
    "    \"\"\"\n",
    "    base_p = find_upwards(base_rel)\n",
    "    local_p = find_upwards(local_rel)\n",
    "\n",
    "    if base_p is None:\n",
    "        print(f\"[AVISO] Arquivo {base_rel} n√£o encontrado a partir de {Path.cwd()}. \"\n",
    "              f\"Crie {base_rel} na raiz do projeto.\")\n",
    "        return {}\n",
    "\n",
    "    config = json.loads(base_p.read_text(encoding=\"utf-8\"))\n",
    "\n",
    "    if local_p and local_p.exists():\n",
    "        local_cfg = json.loads(local_p.read_text(encoding=\"utf-8\"))\n",
    "        config.update(local_cfg)\n",
    "\n",
    "    print(f\"[INFO] Config carregada de: {base_p}\")\n",
    "    if local_p and local_p.exists():\n",
    "        print(f\"[INFO] Overrides locais: {local_p}\")\n",
    "\n",
    "    return config\n",
    "\n",
    "# ---- Carrega configura√ß√µes ----\n",
    "config: Dict[str, Any] = load_config()\n",
    "\n",
    "# ---- Paths (alinhados √† estrutura do template) ----\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "DATA_DIR = PROJECT_ROOT / \"data\"\n",
    "RAW_DIR = DATA_DIR / \"raw\"\n",
    "INTERIM_DIR = DATA_DIR / \"interim\"\n",
    "PROCESSED_DIR = DATA_DIR / \"processed\"\n",
    "REPORTS_DIR = PROJECT_ROOT / \"reports\"\n",
    "ARTIFACTS_DIR = PROJECT_ROOT / \"artifacts\"\n",
    "PRINTS_DIR = PROJECT_ROOT / \"prints\"\n",
    "DASHBOARDS_DIR = PROJECT_ROOT / \"dashboards\"\n",
    "\n",
    "for d in [RAW_DIR, INTERIM_DIR, PROCESSED_DIR, REPORTS_DIR, ARTIFACTS_DIR, PRINTS_DIR, DASHBOARDS_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---- Seed & display ----\n",
    "# Define uma semente global (reprodutibilidade entre execu√ß√µes) e configura par√¢metros de exibi√ß√£o do pandas para facilitar a leitura de tabelas extensas.\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "pd.set_option('display.max_columns', 200)\n",
    "pd.set_option('display.width', 120)\n",
    "\n",
    "# ---- Logging simples ----\n",
    "LOG_FILE = REPORTS_DIR / 'data_preparation.log'\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s | %(levelname)s | %(message)s',\n",
    "    handlers=[\n",
    "        logging.StreamHandler(sys.stdout),\n",
    "        logging.FileHandler(LOG_FILE, encoding='utf-8')\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# ---- Nome dos arquivos principais ----\n",
    "INPUT_FILE = RAW_DIR / 'dataset.csv'\n",
    "OUTPUT_INTERIM = INTERIM_DIR / 'dataset_interim.csv'\n",
    "OUTPUT_PROCESSED = PROCESSED_DIR / 'dataset_processed.csv'\n",
    "\n",
    "logger.info('Project configuration loaded.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0deb8bec",
   "metadata": {},
   "source": [
    "## üß© Fun√ß√µes Utilit√°rias\n",
    "\n",
    "Fun√ß√µes auxiliares para carregamento, inspe√ß√£o, redu√ß√£o de mem√≥ria, tipagem, faltantes e outliers.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18ba2142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilidades gerais de dados\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler, MinMaxScaler\n",
    "\n",
    "def load_csv(filepath: Path, **read_kwargs) -> pd.DataFrame:\n",
    "    \"\"\"Carrega CSV com op√ß√µes expl√≠citas. \n",
    "    - read_kwargs pode receber sep, encoding, dtype, na_values, etc.\n",
    "    \"\"\"\n",
    "    logger.info(f'Loading CSV: {filepath}')\n",
    "    df = pd.read_csv(filepath, **read_kwargs)\n",
    "    return df\n",
    "\n",
    "def save_parquet(df: pd.DataFrame, filepath: Path) -> None:\n",
    "    \"\"\"Salva DataFrame em parquet com feedback.\"\"\"\n",
    "    filepath.parent.mkdir(parents=True, exist_ok=True)\n",
    "    df.to_parquet(filepath, index=False)\n",
    "    logger.info(f'Saved parquet to: {filepath}')\n",
    "\n",
    "def basic_overview(df: pd.DataFrame) -> Dict[str, Any]:\n",
    "    \"\"\"Retorna resumo r√°pido para diagn√≥stico inicial.\"\"\"\n",
    "    info = {\n",
    "        \"shape\": df.shape,\n",
    "        \"columns\": df.columns.tolist(),\n",
    "        \"dtypes\": {c: str(t) for c, t in df.dtypes.items()},\n",
    "        \"memory_mb\": float(df.memory_usage(deep=True).sum() / (1024**2)),\n",
    "        \"na_counts\": df.isna().sum().to_dict()\n",
    "    }\n",
    "    return info\n",
    "\n",
    "def reduce_memory_usage(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Tenta reduzir mem√≥ria para colunas num√©ricas inteiras e floats.\"\"\"\n",
    "    start_mem = df.memory_usage(deep=True).sum() / 1024**2\n",
    "    for col in df.select_dtypes(include=['int64', 'int32', 'int16']).columns:\n",
    "        df[col] = pd.to_numeric(df[col], downcast='integer')\n",
    "    for col in df.select_dtypes(include=['float64', 'float32']).columns:\n",
    "        df[col] = pd.to_numeric(df[col], downcast='float')\n",
    "    end_mem = df.memory_usage(deep=True).sum() / 1024**2\n",
    "    logger.info(f'Memory reduced: {start_mem:.2f}MB -> {end_mem:.2f}MB')\n",
    "    return df\n",
    "\n",
    "def infer_numeric_like(df: pd.DataFrame, columns: Optional[List[str]] = None) -> pd.DataFrame:\n",
    "    \"\"\"Converte colunas com n√∫meros em string (ex.: '1,234' / '1.234,56') para num√©ricas quando poss√≠vel.\"\"\"\n",
    "    target_cols = columns or df.select_dtypes(include=['object']).columns.tolist()\n",
    "    for col in target_cols:\n",
    "        # remove s√≠mbolos comuns e normaliza separadores\n",
    "        series = df[col].astype(str).str.replace(r'[\\s\\$%]', '', regex=True)\n",
    "        series = series.str.replace('.', '', regex=False).str.replace(',', '.', regex=False)\n",
    "        try:\n",
    "            converted = pd.to_numeric(series, errors='raise')\n",
    "            # Se a convers√£o tiver sucesso e gerar varia√ß√£o, assume cast\n",
    "            if converted.notna().sum() > 0 and converted.dtype.kind in 'fi':\n",
    "                df[col] = converted\n",
    "        except Exception:\n",
    "            pass\n",
    "    return df\n",
    "\n",
    "def strip_whitespace(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Remove espa√ßos extras em colunas de texto.\"\"\"\n",
    "    for col in df.select_dtypes(include=['object']).columns:\n",
    "        df[col] = df[col].astype(str).str.strip()\n",
    "    return df\n",
    "\n",
    "def missing_report(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Gera relat√≥rio de faltantes em ordem decrescente.\"\"\"\n",
    "    rep = df.isna().mean().sort_values(ascending=False).rename('missing_rate').to_frame()\n",
    "    rep['missing_count'] = df.isna().sum()\n",
    "    return rep\n",
    "\n",
    "def simple_impute(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Imputa√ß√£o simples: m√©dia (num), moda (cat).\"\"\"\n",
    "    num_cols = df.select_dtypes(include=['number']).columns\n",
    "    cat_cols = df.select_dtypes(exclude=['number']).columns\n",
    "    for c in num_cols:\n",
    "        if df[c].isna().any():\n",
    "            df[c] = df[c].fillna(df[c].median())\n",
    "    for c in cat_cols:\n",
    "        if df[c].isna().any():\n",
    "            df[c] = df[c].fillna(df[c].mode().iloc[0])\n",
    "    return df\n",
    "\n",
    "def detect_outliers_iqr(df: pd.DataFrame, cols: Optional[List[str]] = None) -> pd.DataFrame:\n",
    "    \"\"\"Marca outliers via IQR adicionando colunas booleanas *_is_outlier.\"\"\"\n",
    "    cols = cols or df.select_dtypes(include=['number']).columns.tolist()\n",
    "    for c in cols:\n",
    "        q1 = df[c].quantile(0.25)\n",
    "        q3 = df[c].quantile(0.75)\n",
    "        iqr = q3 - q1\n",
    "        lower = q1 - 1.5 * iqr\n",
    "        upper = q3 + 1.5 * iqr\n",
    "        df[f'{c}_is_outlier'] = (df[c] < lower) | (df[c] > upper)\n",
    "    return df\n",
    "\n",
    "def detect_outliers_zscore(df: pd.DataFrame, threshold: float = 3.0, cols: Optional[List[str]] = None) -> pd.DataFrame:\n",
    "    \"\"\"Marca outliers via Z-score.\"\"\"\n",
    "    cols = cols or df.select_dtypes(include=['number']).columns.tolist()\n",
    "    for c in cols:\n",
    "        mu, sigma = df[c].mean(), df[c].std(ddof=0)\n",
    "        if sigma == 0:\n",
    "            df[f'{c}_is_outlier'] = False\n",
    "        else:\n",
    "            z = (df[c] - mu) / sigma\n",
    "            df[f'{c}_is_outlier'] = z.abs() > threshold\n",
    "    return df\n",
    "\n",
    "def deduplicate_rows(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Remove duplicidades exatas e registra contagem.\"\"\"\n",
    "    before = len(df)\n",
    "    df = df.drop_duplicates()\n",
    "    after = len(df)\n",
    "    logger.info(f'Removed duplicates: {before - after}')\n",
    "    return df\n",
    "\n",
    "def encode_categories(df: pd.DataFrame, encoding: str = 'onehot') -> Tuple[pd.DataFrame, Dict[str, Any]]:\n",
    "    \"\"\"Codifica colunas categ√≥ricas e retorna df transformado + metadados da transforma√ß√£o.\"\"\"\n",
    "    cat_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    meta: Dict[str, Any] = {\"categorical_columns\": cat_cols, \"encoding\": encoding}\n",
    "    if not cat_cols:\n",
    "        return df, meta\n",
    "\n",
    "    if encoding == 'onehot':\n",
    "        encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "        arr = encoder.fit_transform(df[cat_cols])\n",
    "        encoded = pd.DataFrame(arr, columns=encoder.get_feature_names_out(cat_cols), index=df.index)\n",
    "        df = pd.concat([df.drop(columns=cat_cols), encoded], axis=1)\n",
    "        meta['categories_'] = {c: cats.tolist() for c, cats in zip(cat_cols, encoder.categories_)}\n",
    "    elif encoding == 'ordinal':\n",
    "        encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
    "        df[cat_cols] = encoder.fit_transform(df[cat_cols])\n",
    "        meta['categories_'] = {c: list(map(str, cats)) for c, cats in zip(cat_cols, encoder.categories_)}\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported encoding type.\")\n",
    "    return df, meta\n",
    "\n",
    "def scale_numeric(df: pd.DataFrame, method: str = 'standard') -> Tuple[pd.DataFrame, Dict[str, Any]]:\n",
    "    \"\"\"Escala colunas num√©ricas (opcional).\"\"\"\n",
    "    num_cols = df.select_dtypes(include=['number']).columns.tolist()\n",
    "    meta: Dict[str, Any] = {\"numeric_columns\": num_cols, \"scaler\": method}\n",
    "    if not num_cols:\n",
    "        return df, meta\n",
    "\n",
    "    if method == 'standard':\n",
    "        scaler = StandardScaler()\n",
    "    elif method == 'minmax':\n",
    "        scaler = MinMaxScaler()\n",
    "    else:\n",
    "        raise ValueError('Unsupported scaler.')\n",
    "\n",
    "    df[num_cols] = scaler.fit_transform(df[num_cols])\n",
    "    return df, meta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7aca992",
   "metadata": {},
   "source": [
    "## üì• Ingest√£o & Vis√£o R√°pida\n",
    "\n",
    "Carregamento dos dados para criar uma vis√£o geral e um relat√≥rio de faltantes.  \n",
    "> Ajuste `INPUT_FILE` e par√¢metros do `read_csv` conforme a fonte.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9753997b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo: ajuste encoding/sep para seu dataset\n",
    "read_opts = dict(encoding='utf-8', sep=',', low_memory=False)\n",
    "\n",
    "df = load_csv(INPUT_FILE, **read_opts)\n",
    "overview = basic_overview(df)\n",
    "logger.info(json.dumps(overview, indent=2, ensure_ascii=False))\n",
    "print('Vis√£o geral (resumo):')\n",
    "print(json.dumps(overview, indent=2, ensure_ascii=False))\n",
    "\n",
    "missing_df = missing_report(df)\n",
    "display(missing_df.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c23859a",
   "metadata": {},
   "source": [
    "## üß™ Qualidade & Tipagem\n",
    "\n",
    "- Remo√ß√£o de espa√ßos extras em texto.  \n",
    "- Infer√™ncia e **cast** para colunas num√©ricas que chegaram como string.  \n",
    "- **Downcast** para reduzir mem√≥ria.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e16822",
   "metadata": {},
   "outputs": [],
   "source": [
    "if config['strip_whitespace']:\n",
    "    df = strip_whitespace(df)\n",
    "\n",
    "if config['cast_numeric_like']:\n",
    "    df = infer_numeric_like(df)\n",
    "\n",
    "if config['infer_types']:\n",
    "    df = reduce_memory_usage(df)\n",
    "\n",
    "if config['export_interim']:\n",
    "    save_parquet(df, OUTPUT_INTERIM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8569d5cd",
   "metadata": {},
   "source": [
    "## ü©π Tratamento de Valores Faltantes\n",
    "\n",
    "Estrat√©gias comuns: **simples** (mediana/moda) ou mais avan√ßadas (imputadores espec√≠ficos por coluna).  \n",
    "> Mantenha registro do que foi imputado para transpar√™ncia.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04728790",
   "metadata": {},
   "outputs": [],
   "source": [
    "if config['handle_missing']:\n",
    "    print('Relat√≥rio de faltantes (antes):')\n",
    "    display(missing_report(df).head(20))\n",
    "\n",
    "    if config['missing_strategy'] == 'simple':\n",
    "        df = simple_impute(df)\n",
    "    else:\n",
    "        # Espa√ßo para t√©cnicas avan√ßadas personalizadas\n",
    "        pass\n",
    "\n",
    "    print('Relat√≥rio de faltantes (depois):')\n",
    "    display(missing_report(df).head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc838d6",
   "metadata": {},
   "source": [
    "## üö© Detec√ß√£o de Outliers (opcional)\n",
    "\n",
    "Metodologias: **IQR** (robusta) ou **Z-score**.  \n",
    "As colunas booleanas `*_is_outlier` s√£o adicionadas para **inspe√ß√£o** (remover/ajustar √© decis√£o de neg√≥cio).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1338f2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if config['detect_outliers']:\n",
    "    if config['outlier_method'] == 'iqr':\n",
    "        df = detect_outliers_iqr(df)\n",
    "    elif config['outlier_method'] == 'zscore':\n",
    "        df = detect_outliers_zscore(df)\n",
    "    else:\n",
    "        raise ValueError('Unknown outlier method.')\n",
    "\n",
    "    outlier_cols = [c for c in df.columns if c.endswith('_is_outlier')]\n",
    "    logger.info(f'Outlier flags created: {len(outlier_cols)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106b431b",
   "metadata": {},
   "source": [
    "## üß¨ Duplicidades\n",
    "\n",
    "Remo√ß√£o de registros id√™nticos (se fizer sentido).  \n",
    "> Para duplicidades **parciais** (ex.: chaves), trate caso a caso.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8aee5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if config['deduplicate']:\n",
    "    df = deduplicate_rows(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3dc0ea",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Engenharia de Atributos (opcional)\n",
    "\n",
    "Crie atributos √∫teis ao neg√≥cio/modelo: **ratios**, **bins**, **intera√ß√µes** etc.  \n",
    "> Este bloco √© **manual por projeto** ‚Äî adicione suas transforma√ß√µes aqui.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4296c3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if config['feature_engineering']:\n",
    "    # Exemplo gen√©rico (comente/remova conforme o caso):\n",
    "    # if {'col_a','col_b'}.issubset(df.columns):\n",
    "    #     df['a_per_b'] = df['col_a'] / df['col_b'].replace(0, np.nan)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32a1c18",
   "metadata": {},
   "source": [
    "## üìÖ Tratamento de Datas (opcional)\n",
    "\n",
    "- Convers√£o para `datetime`.  \n",
    "- Extra√ß√£o de **ano**, **m√™s**, **dia**, **dia da semana**, **semana do ano**, **m√™s textual**, etc.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c4c15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if config['date_features']:\n",
    "    date_cols = [c for c in df.columns if re.search(r'date|data|dt_', c, re.IGNORECASE)]\n",
    "    for c in date_cols:\n",
    "        try:\n",
    "            df[c] = pd.to_datetime(df[c], errors='coerce', utc=False, infer_datetime_format=True)\n",
    "        except Exception:\n",
    "            pass\n",
    "    # Exemplo de expandir uma coluna de data chamada 'order_date'\n",
    "    if 'order_date' in df.columns and pd.api.types.is_datetime64_any_dtype(df['order_date']):\n",
    "        df['order_year'] = df['order_date'].dt.year\n",
    "        df['order_month'] = df['order_date'].dt.month\n",
    "        df['order_day'] = df['order_date'].dt.day\n",
    "        df['order_dow'] = df['order_date'].dt.dayofweek"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bcf4856",
   "metadata": {},
   "source": [
    "## üìù Tratamento de Texto (opcional)\n",
    "\n",
    "Limpeza, contagens, tamanho de string, presen√ßa de termos-chave etc.  \n",
    "> Use com parcim√¥nia nesta fase; an√°lises espec√≠ficas podem ir para um notebook pr√≥prio.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0f5bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "if config['text_features']:\n",
    "    text_cols = [c for c in df.columns if df[c].dtype == 'object']\n",
    "    for c in text_cols:\n",
    "        df[f'{c}_len'] = df[c].astype(str).str.len()\n",
    "        df[f'{c}_word_count'] = df[c].astype(str).str.split().map(len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc2bc7f",
   "metadata": {},
   "source": [
    "## üî§ Codifica√ß√£o de Categ√≥ricas & üî¢ Escalonamento Num√©rico (opcionais)\n",
    "\n",
    "**Codifica√ß√£o:** One-hot (seguro) ou Ordinal (compacto).  \n",
    "**Escala:** Standard (z-score) ou MinMax (0‚Äì1) ‚Äî √∫til para modelos sens√≠veis √† escala.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ebb07b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding_meta: Dict[str, Any] = {}\n",
    "scaling_meta: Dict[str, Any] = {}\n",
    "\n",
    "if config['encode_categoricals']:\n",
    "    df, encoding_meta = encode_categories(df, encoding=config['encoding_type'])\n",
    "\n",
    "if config['scale_numeric']:\n",
    "    df, scaling_meta = scale_numeric(df, method=config['scaler'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5827a7",
   "metadata": {},
   "source": [
    "## üíæ Exporta√ß√£o de Artefatos\n",
    "\n",
    "Salve dados **intermedi√°rios** e **prontos** para uso em modelagem/visualiza√ß√£o.  \n",
    "Tamb√©m salvamos um **manifest** com metadados das transforma√ß√µes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0339ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "manifest: Dict[str, Any] = {\n",
    "    \"created_at\": datetime.now().isoformat(timespec='seconds'),\n",
    "    \"random_seed\": RANDOM_SEED,\n",
    "    \"config\": config,\n",
    "    \"encoding_meta\": encoding_meta,\n",
    "    \"scaling_meta\": scaling_meta,\n",
    "    \"shape\": df.shape,\n",
    "    \"columns\": df.columns.tolist()\n",
    "}\n",
    "\n",
    "if config['export_interim']:\n",
    "    save_parquet(df, OUTPUT_INTERIM)\n",
    "\n",
    "if config['export_processed']:\n",
    "    save_parquet(df, OUTPUT_PROCESSED)\n",
    "\n",
    "with open(ARTIFACTS_DIR / 'manifest.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(manifest, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print('Arquivos gerados:')\n",
    "print(f'- {OUTPUT_INTERIM if config[\"export_interim\"] else \"(pulado)\"}')\n",
    "print(f'- {OUTPUT_PROCESSED if config[\"export_processed\"] else \"(pulado)\"}')\n",
    "print(f'- {ARTIFACTS_DIR / \"manifest.json\"}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c60f7c2",
   "metadata": {},
   "source": [
    "## ‚úÖ Checkpoint & Pr√≥ximos Passos\n",
    "\n",
    "- **Revise** as colunas derivadas e decis√µes (imputa√ß√£o, outliers, codifica√ß√£o).  \n",
    "- **Documente** no README as escolhas de neg√≥cio e justificativas.  \n",
    "- **Siga** para o pr√≥ximo notebook (ex.: **N2 ‚Äî Modelagem** ou **EDA** detalhada).\n",
    "\n",
    "> Dica: tire **screenshots** de tabelas e distribui√ß√µes e salve em `prints/` para o portf√≥lio.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da717ec1",
   "metadata": {},
   "source": [
    "## üìé Ap√™ndice ‚Äî Anota√ß√µes R√°pidas\n",
    "\n",
    "Use esta se√ß√£o como bloco livre para observa√ß√µes espec√≠ficas do projeto.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
