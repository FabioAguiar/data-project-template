# üß∞ utils/ ‚Äî Utility Toolkit for Data Projects

Este diret√≥rio cont√©m **fun√ß√µes utilit√°rias** usadas pelos notebooks do template para **ingest√£o**, **limpeza**, **engenharia de atributos**, **codifica√ß√£o**, **escala**, **datas**, **texto**, **cat√°logo de DataFrames** e **exporta√ß√£o**.  
O m√≥dulo principal √© **`utils_data.py`**.

> Dica: importar fun√ß√µes espec√≠ficas conforme a etapa do notebook, por exemplo:
>
> ```python
> from utils.utils_data import (
>     load_table_simple, basic_overview, strip_whitespace, infer_numeric_like,
>     simple_impute_with_flags, detect_outliers_iqr, deduplicate_rows,
>     encode_categories_safe, scale_numeric_safe, apply_encoding_and_scaling,
>     parse_dates_with_report, expand_date_features, build_calendar_from,
>     extract_text_features, TableStore, save_table, save_named_interims,
>     list_directory_files
> )
> ```

---

## üì¶ Ingest√£o & Exporta√ß√£o

### `load_csv(filepath, **read_kwargs) -> pd.DataFrame`
Carrega CSV com logging padronizado. Aceita os mesmos par√¢metros do `pandas.read_csv` (ex.: `sep`, `encoding`, `low_memory`).

### `save_parquet(df, filepath) -> None`
Salva um DataFrame em **Parquet**. Cria diret√≥rios pais automaticamente.

### `infer_format_from_suffix(path) -> str`
Deduz o formato do arquivo a partir da extens√£o (`csv`/`parquet`).

### `load_table_simple(path, fmt=None, read_opts=None) -> pd.DataFrame`
Leitura simples e consistente de CSV/Parquet. Se `fmt=None`, detecta pelo sufixo. √ötil na etapa **üì• Ingest√£o & Vis√£o R√°pida**.

### `save_table(df, path) -> None`
Salva respeitando a **extens√£o do caminho**: se terminar em `.csv`, grava CSV; se `.parquet`, grava Parquet.

### `save_named_interims(named_frames, base_dir, fmt="parquet") -> None`
Salva **m√∫ltiplos** DataFrames nomeados em `data/interim/` usando a conven√ß√£o `<nome>_interim.<fmt>`.

### `list_directory_files(dir_path, pattern="*", sort_by="name") -> pd.DataFrame`
Lista arquivos de um diret√≥rio (nome, extens√£o, tamanho e data de modifica√ß√£o). √ötil para configurar rapidamente o bloco `SOURCES`.

---

## üîé Perfil & Otimiza√ß√£o

### `basic_overview(df) -> dict`
Retorna shape, colunas, dtypes, mem√≥ria e contagem de nulos (para log/manifest).

### `reduce_memory_usage(df) -> pd.DataFrame`
Faz **downcast** de inteiros e floats para reduzir uso de mem√≥ria. Loga antes/depois.

### `strip_whitespace(df) -> pd.DataFrame`
Remove espa√ßos excedentes em colunas textuais (`object`).

### `infer_numeric_like(df, columns=None, min_ratio=0.9, create_new_col_when_partial=True, blacklist=None, whitelist=None) -> (df, report_df)`
Converte strings ‚Äúparecidas com n√∫mero‚Äù em valores num√©ricos, com auditoria:
- Detecta porcentagens (`%`) e normaliza separadores (`1.234,56` ‚Üî `1,234.56`).
- Se **`ratio >= min_ratio`**, sobrescreve a coluna; caso parcial, cria `<col>_num` (se habilitado).
- Retorna um **relat√≥rio** com `column`, `action`, `ratio`, `converted`, `non_convertible`, `examples`.

---

## ü©π Faltantes & Outliers & Duplicatas

### `missing_report(df) -> pd.DataFrame`
Tabela com `missing_rate` e `missing_count` por coluna.

### `simple_impute(df) -> pd.DataFrame`
Imputa√ß√£o ‚Äúsimples‚Äù: num√©ricos ‚Üí mediana; categ√≥ricos ‚Üí moda.

### `simple_impute_with_flags(df) -> pd.DataFrame`
Igual ao anterior, mas adiciona **flags booleanas** `was_imputed_<col>` marcando linhas preenchidas (rastreabilidade).

### `detect_outliers_iqr(df, cols=None) -> pd.DataFrame`
Cria colunas `<col>_is_outlier` usando m√©todo **IQR** (robusto a assimetrias).

### `detect_outliers_zscore(df, threshold=3.0, cols=None) -> pd.DataFrame`
Cria colunas `<col>_is_outlier` a partir de **Z-score** (assume distribui√ß√£o ~normal).

### `deduplicate_rows(df, subset=None, keep="first", log_path=None, return_report=False)`
Remove duplicatas e, opcionalmente, **loga** as linhas duplicadas em CSV.
- `subset`: colunas que definem a chave; `None` = linha inteira.
- `keep`: `"first" | "last" | False` (remove todas as repeti√ß√µes).
- `return_report=True` retorna `(df_limpo, dups_df, resumo_df)`.

---

## üî§ Categ√≥ricas & üî¢ Num√©ricas

> Use as vers√µes **_safe_** abaixo para mais controle (exclus√µes e avisos).

### `encode_categories(df, encoding="onehot") -> (df, meta)`
Wrapper simples (usa scikit-learn). Converte todas as categ√≥ricas do DF passado.
- `encoding="onehot" | "ordinal"`
- `meta` inclui mapeamentos de categorias.

### `scale_numeric(df, method="standard") -> (df, meta)`
Padroniza/normaliza **todas** as colunas num√©ricas do DF passado.
- `method="standard" | "minmax"`

### `encode_categories_safe(df, method="onehot", exclude_cols=None, high_card_threshold=50) -> (df, meta)`
Codifica√ß√£o com **exclus√£o de colunas** (ex.: `["Churn","customerID"]`) e **aviso de alta cardinalidade**.
- `meta`: colunas categ√≥ricas tratadas, exclu√≠das e aviso de cardinalidade.

### `scale_numeric_safe(df, method="standard", exclude_cols=None, only_continuous=True) -> (df, meta)`
Escala **apenas** as num√©ricas **cont√≠nuas** (ignora dummies/booleanas) e permite excluir colunas-alvo.

### `apply_encoding_and_scaling(df, encode_cfg=None, scale_cfg=None) -> (df, encoding_meta, scaling_meta)`
Orquestra **encode ‚Üí scale** com configs:
- `encode_cfg = {enabled, type, exclude_cols, high_card_threshold}`
- `scale_cfg  = {enabled, method, exclude_cols, only_continuous}`

---

## üìÖ Datas

### `detect_date_candidates(df, pattern) -> list[str]`
Encontra colunas candidatas a data por **regex** ou dtype datetime j√° existente.

### `_maybe_to_datetime(s, dayfirst, utc, formats) -> pd.Series` _(interno)_
Tenta converter com formatos expl√≠citos; se falhar, usa fallback gen√©rico.

### `parse_dates_with_report(df, cfg) -> (df, parse_report, parsed_cols)`
Converte colunas de data e retorna relat√≥rio com:
- `column`, `parsed_ratio`, `converted` (aceita se `parsed_ratio >= min_ratio`).
- `cfg`: `detect_regex`, `explicit_cols`, `dayfirst`, `utc`, `formats`, `min_ratio`.

### `expand_date_features(df, cols, features=None, prefix_mode="auto", fixed_prefix=None) -> list[str]`
Gera features como `*_year`, `*_month`, `*_day`, `*_dow`, `*_quarter`, `*_week`, `*_is_month_start`, `*_is_month_end`.

### `build_calendar_from(df, date_col, freq="D") -> pd.DataFrame`
Cria uma **dimens√£o calend√°rio** (`dim_date`) com atributos de data entre o min/max observados.

---

## üìù Texto

### `extract_text_features(df, lower=True, strip_collapse_ws=True, keywords=None, blacklist=None, export_summary=False, summary_dir=None) -> (df, summary_df)`
Extrai m√©tricas simples de colunas textuais (`object`):
- Comprimento, contagem de palavras, contagem de **letras** e **d√≠gitos**.
- Flags de **palavras-chave** `*_has_<kw>` (case-insensitive).
- `summary_df` lista colunas processadas e flags criadas; pode salvar CSV.

---

## üìö Cat√°logo de DataFrames

### `TableStore`
Cat√°logo leve para gerenciar m√∫ltiplos DataFrames nomeados com um **‚Äúcurrent‚Äù**.
- **Principais m√©todos**
  - `add(name, df, set_current=False)` ‚Äî registra/atualiza.
  - `get(name=None)` ‚Äî obt√©m o df de `name` (ou o atual).
  - `use(name)` ‚Äî torna `name` atual e retorna o df.
  - `list()` ‚Äî invent√°rio (nome, linhas, colunas, mem√≥ria).
  - `rename(old, new)` / `drop(name)`.
- **Acesso estilo dicion√°rio:** `T["features_v1"]`.

---

## üß™ Exemplos r√°pidos

### 1) Ingest√£o + vis√£o geral
```python
df = load_table_simple(RAW_DIR / "dataset.csv", read_opts={"encoding":"utf-8", "sep": ","})
print(basic_overview(df))
```

### 2) Limpeza e tipagem
```python
df = strip_whitespace(df)
df, rep = infer_numeric_like(df, min_ratio=0.9, blacklist=["customerID"])
```

### 3) Faltantes, outliers e duplicatas
```python
df = simple_impute_with_flags(df)
df = detect_outliers_iqr(df)
df = deduplicate_rows(df, subset=["customerID"], keep="first", log_path=REPORTS_DIR/"duplicates.csv")
```

### 4) Datas + calend√°rio
```python
cfg = {"detect_regex": r"(date|data|_dt$|_date$)", "min_ratio": 0.8}
df, parse_report, parsed = parse_dates_with_report(df, cfg)
created = expand_date_features(df, parsed)
dim_date = build_calendar_from(df, date_col="order_date", freq="D")
```

### 5) Texto
```python
df, text_sum = extract_text_features(df, keywords=["error", "cancel"], blacklist=["customerID"])
```

### 6) Encode & Scale (safe)
```python
ENCODE_CFG = {"enabled": True, "type": "onehot", "exclude_cols": ["Churn","customerID"], "high_card_threshold": 50}
SCALE_CFG  = {"enabled": True, "method": "standard", "exclude_cols": ["Churn"], "only_continuous": True}
df, enc_meta, scl_meta = apply_encoding_and_scaling(df, ENCODE_CFG, SCALE_CFG)
```

### 7) Cat√°logo de DataFrames
```python
T = TableStore(initial={"main": df}, current="main")
df = T.get()
T.add("features_v1", df, set_current=True)
display(T.list())
```

---

## ‚úÖ Depend√™ncias
- `pandas`, `numpy`
- `scikit-learn` (para codifica√ß√£o/escala)
- Python ‚â• 3.10 recomendado

---

## üîñ Exporta√ß√µes (API do m√≥dulo)
As principais fun√ß√µes/classes expostas via `__all__` incluem:
- Ingest√£o/Exporta√ß√£o: `load_csv`, `save_parquet`, `save_table`, `load_table_simple`, `infer_format_from_suffix`, `save_named_interims`
- Perfil/Otimiza√ß√£o: `basic_overview`, `reduce_memory_usage`, `strip_whitespace`, `infer_numeric_like`, `missing_report`, `simple_impute`, `simple_impute_with_flags`
- Qualidade: `detect_outliers_iqr`, `detect_outliers_zscore`, `deduplicate_rows`
- Categ/Num: `encode_categories`, `scale_numeric`, `encode_categories_safe`, `scale_numeric_safe`, `apply_encoding_and_scaling`
- Datas: `parse_dates_with_report`, `expand_date_features`, `build_calendar_from`, `detect_date_candidates`
- Texto: `extract_text_features`
- Cat√°logo: `TableStore`
- Utilidade: `list_directory_files`

---

## üß≠ Conven√ß√µes
- **Sufixos de auditoria:** `_is_outlier`, `was_imputed_<col>`, `<col>_num`, `*_has_<kw>`.
- **Logs** s√£o emitidos pelo `logger` do m√≥dulo (gravados em `reports/data_preparation.log` quando configurado no notebook).
- Fun√ß√µes ‚Äú_safe_‚Äù priorizam **previsibilidade** e **controle expl√≠cito** ao custo de mais par√¢metros.
