# ğŸ§© Data Project Template

Modelo-base para projetos de **anÃ¡lise** e **engenharia de dados**, com foco em **clareza**, **organizaÃ§Ã£o** e **reprodutibilidade**.  
Inclui pipeline de preparaÃ§Ã£o, utilitÃ¡rios prontos em `utils/`, configuraÃ§Ã£o declarativa via `config/` e exportaÃ§Ã£o de artefatos.

---

## ğŸ“ Estrutura do RepositÃ³rio

```
data-project-template/
â”œâ”€â”€ artifacts/        # saÃ­das auxiliares do projeto (dim_date, mÃ©tricas, etc.)
â”œâ”€â”€ config/           # defaults.json (obrigatÃ³rio) e local.json (opcional, overrides)
â”œâ”€â”€ dashboards/       # arquivos de dashboards (Power BI, etc.)
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/          # dados brutos 
â”‚   â”œâ”€â”€ interim/      # camadas intermediÃ¡rias apÃ³s limpeza/padrÃµes
â”‚   â””â”€â”€ processed/    # dataset final para modelagem/visualizaÃ§Ã£o
â”œâ”€â”€ notebooks/        # Jupyter Notebooks do fluxo (N1â€¦Nn)
â”œâ”€â”€ prints/           # screenshots/figuras para documentaÃ§Ã£o
â”œâ”€â”€ reports/          # logs, relatÃ³rios CSV/HTML, PDF e manifest.json
â”œâ”€â”€ utils/            # funÃ§Ãµes reutilizÃ¡veis (utils_data.py, etc.)
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
```


---

## ğŸš€ Fluxo de Trabalho no notebook 01_data_preparation_template

1. **ConfiguraÃ§Ã£o do Projeto**  
   - Descoberta da raiz, carregamento de `config/defaults.json` (e `local.json` se existir).  
   - DefiniÃ§Ã£o de caminhos `data/`, `reports/`, `artifacts/` e logging em `reports/data_preparation.log`.

2. **ConfiguraÃ§Ã£o de Fontes**  
   - Defina `SOURCES` (caminho, formato e opÃ§Ãµes) e `MAIN_SOURCE`.  
   - Opcional: `MERGE_STEPS` para encadear *joins* (com checagem bÃ¡sica de chaves).

3. **IngestÃ£o & VisÃ£o RÃ¡pida**  
   - Leitura de cada fonte (`load_table_simple`) + `basic_overview` e `missing_report`.  
   - ExecuÃ§Ã£o de *merge chain* quando configurado.

4. **CatÃ¡logo de DataFrames**  
   - `TableStore` centraliza mÃºltiplos `DataFrames` nomeados.  
   - ConvenÃ§Ã£o: manter um `df` â€œativoâ€ via `T.get()` ou `T.use("nome")`.

5. **Qualidade & Tipagem**  
   - `strip_whitespace` â†’ limpeza leve de texto.  
   - `infer_numeric_like` â†’ converte strings numÃ©ricas respeitando *ratio* mÃ­nimo, com *report*.  
   - `reduce_memory_usage` â†’ *downcast* numÃ©rico e relatÃ³rio de memÃ³ria.  
   - ExportaÃ§Ã£o **interim** (quando habilitado).

6. **PadronizaÃ§Ã£o CategÃ³rica (prÃ©-engenharia)**  
   - NormalizaÃ§Ãµes explÃ­citas e simples (ex.: `"No internet service" â†’ "No"`).  
   - Mantida prÃ³xima da etapa anterior por depender da detecÃ§Ã£o de tipos/valores.

7. **Tratamento de Faltantes**  
   - `simple_impute_with_flags` (mediana/moda) + colunas `was_imputed_<col>`.  
   - TransparÃªncia e rastreabilidade dos preenchimentos.

8. **DetecÃ§Ã£o de Outliers (opcional)**  
   - **IQR** ou **Z-score** gerando apenas *flags* `*_is_outlier` (decisÃ£o de negÃ³cio fica fora).

9. **Duplicidades**  
   - `deduplicate_rows` com suporte a `subset`, polÃ­tica `keep` e *log* CSV opcional de duplicatas.  
   - Pode retornar relatÃ³rio de chaves duplicadas.

10. **Tratamento de Datas**  
    - `parse_dates_with_report` detecta/forÃ§a colunas de data e audita *parsed_ratio*.  
    - `expand_date_features` cria `*_year`, `*_month`, `*_week`, `*_is_month_*` etc.  
    - CriaÃ§Ã£o de **dim_date** com `build_calendar_from` (opcional).

11. **Tratamento de Texto (opcional)**  
    - `extract_text_features`: tamanho, contagem de palavras/letras/dÃ­gitos e *keywords*.  
    - OpÃ§Ã£o de exportar *summary* em `reports/text_features/`.

12. **CodificaÃ§Ã£o & Escalonamento (opcionais)**  
    - `apply_encoding_and_scaling`: *wrapper* que orquestra  
      `encode_categories_safe` (one-hot/ordinal com exclusÃµes e alerta de cardinalidade)  
      e `scale_numeric_safe` (standard/minmax apenas em contÃ­nuas, se desejado).

13. **ExportaÃ§Ã£o de Artefatos**  
    - `save_table` respeita extensÃ£o (`.csv`/`.parquet`) para **interim**/**processed**.  
    - GeraÃ§Ã£o de `reports/manifest.json` com shape, colunas e metadados de encode/scale.

---

## âš™ï¸ ConfiguraÃ§Ã£o via `config/`

- **`defaults.json`**: parÃ¢metros padrÃ£o (obrigatÃ³rio).  
- **`local.json`**: overrides por projeto/ambiente (opcional).  
- As *flags* mais usadas:  
  - `infer_types`, `cast_numeric_like`, `strip_whitespace`  
  - `handle_missing` + `missing_strategy`  
  - `detect_outliers` + `outlier_method`  
  - `deduplicate` (+ subset/keep/log)  
  - `normalize_categories`  
  - `date_features`, `text_features`, `feature_engineering`  
  - `encode_categoricals` + `encoding_type`  
  - `scale_numeric` + `scaler`  
  - `export_interim`, `export_processed`

> As configuraÃ§Ãµes ativas sÃ£o registradas no log e no `manifest.json`.

---

## ğŸ§° Principais UtilitÃ¡rios (`utils/utils_data.py`)

- **IngestÃ£o**: `infer_format_from_suffix`, `load_table_simple`, `merge_chain`  
- **Qualidade/Tipos**: `strip_whitespace`, `infer_numeric_like`, `reduce_memory_usage`, `missing_report`, `simple_impute_with_flags`  
- **Outliers & Duplicidades**: `detect_outliers_iqr`, `detect_outliers_zscore`, `deduplicate_rows`  
- **Datas**: `parse_dates_with_report`, `expand_date_features`, `build_calendar_from`  
- **Texto**: `extract_text_features`  
- **Encode & Scale**: `encode_categories_safe`, `scale_numeric_safe`, `apply_encoding_and_scaling`  
- **CatÃ¡logo**: `TableStore`, `save_named_interims`  
- **Arquivos**: `save_table`, `save_parquet`, `list_directory_files`

Um README detalhado dos utilitÃ¡rios estÃ¡ em `utils/UTILS_README.md`.

---

## ğŸ§ª Rodando o Template (resumo)

1. Coloque seus arquivos em `data/raw/`.  
2. (Opcional) Execute no notebook a listagem de arquivos: `list_directory_files(RAW_DIR)` para escolher *sources*.  
3. Configure `SOURCES`, `MAIN_SOURCE` e (se necessÃ¡rio) `MERGE_STEPS`.  
4. Siga as cÃ©lulas do pipeline (N1 â€” PreparaÃ§Ã£o de Dados).  
5. Exporte *interim*/*processed* + `manifest.json`.

---

## ğŸ”’ Boas PrÃ¡ticas

- **NÃ£o comite dados sensÃ­veis**. Prefira *placeholders* e `.gitignore`.  
- Documente normalizaÃ§Ãµes e decisÃµes de negÃ³cio no README do projeto.  
- Use `local.json` para ajustes de ambiente sem tocar o template.  
- Registre mudanÃ§as relevantes nos logs e no `manifest.json`.

---

## ğŸ“ LicenÃ§a & CrÃ©ditos

- LicenÃ§a: MIT (ajuste conforme sua necessidade).  
- Template montado para estudos/portfÃ³lio e rÃ¡pido *bootstrap* de projetos de dados.

## ğŸš€ Getting Started

### 1) Ambiente
```bash
python -m venv .venv
# Linux/macOS
source .venv/bin/activate
# Windows (Powershell)
# .venv\Scripts\Activate.ps1

pip install -r requirements.txt  # ou instale as libs do seu stack padrÃ£o
```

### 2) Estrutura mÃ­nima
Coloque seus arquivos de entrada em `data/raw/`. Exemplo:
```
data/raw/
â”œâ”€â”€ input.csv
â””â”€â”€ customers_2025-10-01.csv
```

### 3) ConfiguraÃ§Ãµes
- O arquivo `config/defaults.json` contÃ©m as flags padrÃ£o do pipeline.
- Para ajustes locais (sem mexer nos defaults), crie `config/local.json`. Exemplo:
```json
{
  "text_features": true,
  "export_processed": true,
  "scale_numeric": true,
  "scaler": "minmax",
  "normalize_categories": true
}
```
> O projeto faz *merge* de `defaults.json` com `local.json` (local sobrepÃµe).

### 4) ExecuÃ§Ã£o do N1 (PreparaÃ§Ã£o de Dados)
Abra e rode o notebook:
```
notebooks/01_data_preparation.ipynb
```
SaÃ­das esperadas:
- IntermediÃ¡rios em `data/interim/` (se habilitado)
- Processados em `data/processed/` (se habilitado)
- RelatÃ³rios e logs em `reports/`

### 5) Dicas
- Mantenha apenas uma **fonte canÃ´nica** de dados brutos em `data/raw/`.
- Use nomes descritivos e com datas (`snake_case` + `YYYY-MM-DD`).

---

### ğŸ§­ Filosofia de normalizaÃ§Ã£o categÃ³rica
Por padrÃ£o, mantemos os rÃ³tulos exatamente como estÃ£o nos dados brutos. A normalizaÃ§Ã£o sÃ³ ocorre quando
`normalize_categories = true`, garantindo **controle explÃ­cito** e evitando perda de semÃ¢ntica (ex.: diferenÃ§as sutis
de grafia que carregam significado). Essa regra torna a transformaÃ§Ã£o **previsÃ­vel** e **auditÃ¡vel** â€” vocÃª decide quando
e como normalizar.
